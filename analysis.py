# -*- coding: utf-8 -*-
"""analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cTRW5n3t3G9h8FWJgHG9C-rzyG1GZFaA
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas beautifulsoup4 requests openpyxl

import pandas as pd
def read_input_file(file_path='Input.xlsx'):

    df = pd.read_excel(file_path)
    if 'URL_ID' not in df.columns or 'URL' not in df.columns:
        raise ValueError("Input.xlsx error")
    return df

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas requests beautifulsoup4 openpyxl

# Commented out IPython magic to ensure Python compatibility.
# %pip install selenium

import os
article_dir = '/content/drive/MyDrive/Text-Analysis/articles'
results = []
for filename in os.listdir(article_dir):
    if filename.endswith('.txt'):
        url_id = filename.replace('.txt', '')
        with open(os.path.join(article_dir, filename), 'r', encoding='utf-8') as file:
            text = file.read()

            metrics = analyze_text(text)
            metrics['URL_ID'] = url_id
            results.append(metrics)

import os
import requests
import pandas as pd
from bs4 import BeautifulSoup

# Load input Excel file
input_path = '/content/drive/MyDrive/Input.xlsx'
df_input = pd.read_excel(input_path)
df_input.columns = df_input.columns.str.strip()

articles_folder = '/content/drive/MyDrive/Text-Analysis/articles'
os.makedirs(articles_folder, exist_ok=True)
def extract_article(url):
    try:
        response = requests.get(url, timeout=10)
        if response.status_code != 200:
            print(f" Failed to fetch {url}")
            return None, None

        soup = BeautifulSoup(response.content, 'html.parser')
        title = soup.find('h1')
        title_text = title.get_text().strip() if title else "Not Found"
        article_body = ""
        article_tag = soup.find('article')
        if article_tag:
            paragraphs = article_tag.find_all('p')
        else:

            div_tag = soup.find('div', class_=lambda x: x and 'content' in x.lower() or 'article' in x.lower())
            if div_tag:
                paragraphs = div_tag.find_all('p')
            else:

                paragraphs = soup.find_all('p')

        article_body = '\n'.join(p.get_text().strip() for p in paragraphs if p.get_text().strip())

        return title_text, article_body

    except Exception as e:
        print(f" Error for URL {url}: {e}")
        return None, None

# Loop
for idx, row in df_input.iterrows():
    url_id = str(row['URL_ID']).strip()
    url = row['URL'].strip()

    print(f" Processing: {url} â†’ {url_id}.txt")
    title, content = extract_article(url)

    if content:
        file_path = os.path.join(articles_folder, f"{url_id}.txt")
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"{title}\n\n{content}")
        print(f" Saved: {file_path}")
    else:
        print(f" Skipped: {url_id}")

print("extracted articles.")

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas requests beautifulsoup4 openpyxl

# Commented out IPython magic to ensure Python compatibility.
# %pip install textstat nltk

import nltk
nltk.download('punkt')
nltk.download('stopwords')

# Commented out IPython magic to ensure Python compatibility.
# %pip install textstat

import nltk
# Correct NLTK downloads
nltk.download('punkt')
nltk.download('stopwords')

import os
import pandas as pd
import nltk
import textstat
import re
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords

# Correct NLTK downloads
nltk.download('punkt')
nltk.download('stopwords')

def load_sentiment_words():
   with open('/content/drive/MyDrive/MasterDictionary/positive-words.txt', encoding='ISO-8859-1') as f:
    positive_words = set(f.read().split())


    with open('/content/drive/MyDrive/MasterDictionary/negative-words.txt', encoding='ISO-8859-1') as f:
        negative_words = set(f.read().split())

    return positive_words, negative_words

positive_words, negative_words = load_sentiment_words()

# Text analysis
def analyze_text(text):
    sentences = sent_tokenize(text)
    words = word_tokenize(text)
    words = [word.lower() for word in words if word.isalpha()]
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word not in stop_words]

    word_count = len(filtered_words)
    sentence_count = len(sentences)
    complex_words = [word for word in filtered_words if textstat.syllable_count(word) > 2]
    complex_word_count = len(complex_words)

    positive_score = sum(1 for word in filtered_words if word in positive_words)
    negative_score = sum(1 for word in filtered_words if word in negative_words)

    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 1e-6)
    subjectivity_score = (positive_score + negative_score) / (word_count + 1e-6)

    avg_sentence_length = word_count / (sentence_count + 1e-6)
    percentage_complex_words = complex_word_count / (word_count + 1e-6)
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

    syllables_per_word = sum(textstat.syllable_count(word) for word in filtered_words) / (word_count + 1e-6)
    personal_pronouns = len(re.findall(r'\b(I|we|my|ours|us)\b', text, re.I))
    avg_word_length = sum(len(word) for word in filtered_words) / (word_count + 1e-6)

    return {
        'POSITIVE SCORE': positive_score,
        'NEGATIVE SCORE': negative_score,
        'POLARITY SCORE': polarity_score,
        'SUBJECTIVITY SCORE': subjectivity_score,
        'AVG SENTENCE LENGTH': avg_sentence_length,
        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,
        'FOG INDEX': fog_index,
        'AVG NUMBER OF WORDS PER SENTENCE': avg_sentence_length,
        'COMPLEX WORD COUNT': complex_word_count,
        'WORD COUNT': word_count,
        'SYLLABLE PER WORD': syllables_per_word,
        'PERSONAL PRONOUNS': personal_pronouns,
        'AVG WORD LENGTH': avg_word_length
    }

# load input to excel
df_input = pd.read_excel('/content/drive/MyDrive/Input.xlsx')
df_input.columns = df_input.columns.str.strip()

# Set article
articles_folder = '/content/drive/MyDrive/Text-Analysis/articles'

# Analyze articles
results = []
for filename in os.listdir(articles_folder):
    if filename.endswith('.txt'):
        url_id = filename.replace('.txt', '')
        with open(os.path.join(articles_folder, filename), 'r', encoding='utf-8') as f:
            content = f.read()
            metrics = analyze_text(content)
            metrics['URL_ID'] = url_id
            results.append(metrics)
df_results = pd.DataFrame(results)

# Merge using 'URL_ID'
df_final = pd.merge(df_input, df_results, on='URL_ID', how='left')
df_final.to_excel('/content/drive/MyDrive/Text-Analysis/Output Data Structure.xlsx', index=False)

# Commented out IPython magic to ensure Python compatibility.
# %pip install textstat

import os
import pandas as pd
import nltk
import textstat
import re
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords


nltk.download('punkt_tab')
nltk.download('stopwords')

# Load master dictionaries
def load_sentiment_words():
    with open('/content/drive/MyDrive/MasterDictionary/positive-words.txt', encoding='ISO-8859-1') as f:
        positive_words = set(f.read().split())

    with open('/content/drive/MyDrive/MasterDictionary/negative-words.txt', encoding='ISO-8859-1') as f:
        negative_words = set(f.read().split())

    return positive_words, negative_words

positive_words, negative_words = load_sentiment_words()

# Analysis function
def analyze_text(text):
    sentences = sent_tokenize(text)
    words = word_tokenize(text)
    words = [word.lower() for word in words if word.isalpha()]
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word not in stop_words]

    word_count = len(filtered_words)
    sentence_count = len(sentences)
    complex_words = [word for word in filtered_words if textstat.syllable_count(word) > 2]
    complex_word_count = len(complex_words)

    positive_score = sum(1 for word in filtered_words if word in positive_words)
    negative_score = sum(1 for word in filtered_words if word in negative_words)

    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 1e-6)
    subjectivity_score = (positive_score + negative_score) / (word_count + 1e-6)

    avg_sentence_length = word_count / (sentence_count + 1e-6)
    percentage_complex_words = complex_word_count / (word_count + 1e-6)
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

    syllables_per_word = sum(textstat.syllable_count(word) for word in filtered_words) / (word_count + 1e-6)
    personal_pronouns = len(re.findall(r'\b(I|we|my|ours|us)\b', text, re.I))
    avg_word_length = sum(len(word) for word in filtered_words) / (word_count + 1e-6)

    return {
        'POSITIVE SCORE': positive_score,
        'NEGATIVE SCORE': negative_score,
        'POLARITY SCORE': polarity_score,
        'SUBJECTIVITY SCORE': subjectivity_score,
        'AVG SENTENCE LENGTH': avg_sentence_length,
        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,
        'FOG INDEX': fog_index,
        'AVG NUMBER OF WORDS PER SENTENCE': avg_sentence_length,
        'COMPLEX WORD COUNT': complex_word_count,
        'WORD COUNT': word_count,
        'SYLLABLE PER WORD': syllables_per_word,
        'PERSONAL PRONOUNS': personal_pronouns,
        'AVG WORD LENGTH': avg_word_length
    }

#input data
df_input = pd.read_excel('/content/drive/MyDrive/Input.xlsx')
df_input.columns = df_input.columns.str.strip()

articles_folder = '/content/drive/MyDrive/Text-Analysis/articles'
results = []
for filename in os.listdir(articles_folder):
    if filename.endswith('.txt'):
        url_id = filename.replace('.txt', '')

        with open(os.path.join(articles_folder, filename), 'r', encoding='utf-8') as f:

            content = f.read()
            metrics = analyze_text(content)
            metrics['URL_ID'] = url_id
            results.append(metrics)

df_results = pd.DataFrame(results)


df_final = pd.merge(df_input, df_results, on='URL_ID', how='left')
df_final.to_excel('Output Data Structure.xlsx', index=False)

display(df_final)